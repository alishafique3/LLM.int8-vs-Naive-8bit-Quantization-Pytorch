# LLM.int8 vs Naive 8-bit Weight Quantization Using Pytorch
[In progress]
