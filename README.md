# Naive 8-bit Weight Quantization vs LLM.int8 Using Pytorch
[In progress]
